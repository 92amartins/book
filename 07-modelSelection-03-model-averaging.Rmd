## Bayesian Model Averaging

In the last section, we explored model uncertainty using posterior probabilities of models based on BIC. In this section, we will continue the kid's cognitive score example to see how to obtain an Bayesian model averaging results using model posterior probabilities 
### Visualizing Model Uncertainty

Recall that in the last section, we used the `bas.lm` function in the `BAS` package to obtain posterior probabilities of all models in the kid's cognitive score example. 
$$ \text{kid_score} ~\sim~ \text{hq} + \text{IQ} + \text{work} + \text{age} $$

We have found the posterior distribution under model uncertainty using all possible combinations of the predictors, the mom's high school status `hs`, mom's IQ score `IQ`, whether the mom worked during the first three years of the kid's life `work`, and mom's age `age`. With 4 predictors, there are $2^4 = 16$ possible models. In general, for linear regression model
$$ y_i = \beta_0+\beta_1x_{1,i} + \cdots + \beta_px_{p,i},\qquad i = 1, \cdots,n$$
that has $p$ predictors, there will be in total $2^p$ possible models.

We can also visualize model uncertainty from the `bas` object `cog_bas`. 

```{r read-data}
# Load the library in order to read in data from website
library(foreign)    

# Read in cognitive score data set and process data tranformations
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")

cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")

library(BAS)
cog_bas = bas.lm(kid_score ~ hs + IQ + work + age, data = cognitive,
                 prior = "BIC",
                 modelprior = uniform())
```

In R, the image function may be used to create an image of the model space that looks like a crossword puzzle. 
```{r visualize}
image(cog_bas, rotate = F)
```

To obtain a clearer view for comparison of models, we did not rotate the image. Here, the predictors, including the intercept, are on the $y$-axis, while the $x$-axis corresponds to each different model. Each vertical column corresponds to one model. For variables that are not included, they will be represented by a black block. For example, model 1 includes the intercept, `hs`, and `IQ`, but not `work` or `age`. These models are ordered according to the log of posterior odds???? (check notes. If odds, over which model). The colors are proportional to the log of the posterior probabilities. Models with same colors have similar posterior probabilities. This allows us to view models that are clustered together, where the differences are not worth a bare mention.

If we view the image by rows, we can see whether one variable is included in the particular model. For each variable, there are only 8 models in which it will appear. For example, we see that `IQ` appears in all the top 8 models with larger posterior probabilities, but not the last 8 models. The `image` function will show up to 20 models by default.

### Bayesian Model Averaging Using Posterior Probabilities

Once we have obtained the posterior probabilities of each model, we may view them as our weights and make inference using these weights. Models with higher posterior probabilities receive higher weights, while models with lower posterior probabilities receive lower weights. We may average a quantity using the posterior probabilities of models as weights over all models, which gives the name "Bayesian  Model Averaging" (BMA). For example, the probability of the next prediction $\hat{Y}^*$ after seeing the data can be calculated as a "weighted average" of the prediction of next observation $\hat{Y}^*$ under each model $M_j$, with the posterior probabilities of $M_j$ being the "weights"
$$ \hat{Y}^* = \sum_{j=1}^{2^p}\hat{Y}^*_j\ p(M_j~|~\text{data}), $$
where $\hat{Y}^*_j$ is the prediction under model $M_j$.

In general, for a quantity of interest $\Delta$, which may be $Y^*$, the next observation, $\beta_j$, the coefficient of a variable, even $p(\beta_j~|~\text{data})$, the posterior probability of $\beta_j$ after seeing the data, its posterior probability of seeing the data can be calculated using the formula

$$ p(\Delta~|~\text{data}) = \sum_{j=1}^{2^p}p(\Delta~|~ M_j,\ \text{data})p(M_j~|~\text{data}) $$

This formula is similar to those in Week 1 when we used posterior probabilities of the two competing hypotheses to calculate the predictive probability of an event. For example, when we have two hypothese about the probability of getting heads in a coin flip $p$:
$$ H_1: p=p_0,\qquad \text{vs}\qquad H_2: p\neq p_0. $$
After obtaining the posterior probabilities seeing the data $P(H_1~|~\text{data})$ and $P(H_2~|~\text{data})$, we can calculate the predictive probability of getting the next coin using the two posterior probabilities as weights:
$$ P(\text{next head}~|~\text{data}) = P(\text{next head}~|~H_1)P(H_1~|~\text{data})+P(\text{next head}~|~H_2)P(H_2~|~\text{data}). $$

Moreover, the expected value of $\Delta$ can also be obtained by a weighted average of conditional expected values on each model
$$ E[\Delta~|~\text{data}] = \sum_{j=1}^{2^p}E[\Delta~|~M_j,\ \text{data}]p(M_j~|~\text{data}).$$


Since the weights $p(M_j~|~\text{data})$ are probabilities and have to sum to one, if the best model has posterior probability one, all of the weights will be placed on that single best model, and using BMA would be equivalent to selecting the best model with the highest posterior probability. However, if there's several models that receive substantial probability, they would all be included in the inference and account for the uncertainty about the true model. 

### Coefficient Distributions under BMA

<!--
We can obtain summaries for the coefficients. This produces a table that is a Bayesian analogue to the regression coefficient summary from LM. The first column is the posterior mean of the coefficient, or the value that we expect under Bayesian model averaging, which would be used for prediction. The posterior SD, or standard deviation, provides a measure of the variability of the coefficient. And an approximate range of plausible values for each of the coefficients may be attained via the empirical rule, using the mean plus or minus two standard deviations. 
4:35
This applies if the posterior distribution is symmetric or unimodal. Last, we have the posterior probability that the coefficient is non-zero, which replaces the p value. Here, we can see that we are virtually certain that mom's IQ should be included, with a probability approximately 1. We're 61% sure that mom's high school should be included, while the probability that working and age are non-zero, taking into account uncertainty in the other variables, are 11% and 7% respectively. From the law of total probability, this means that there is a 0.93 probability that the coefficient for age is 0 after adjusting for all of the other variables. Now that we've looked at the collection of models, let's turn to visualizing plausible values for the coefficients, taking into account that there is uncertainty about the best model. 
5:30
This plot of the posterior distributions for each of the regression coefficients is displayed in a two by two table. Let's focus on the plot for mom's high school status. The vertical bar represents the posterior probability that the coefficient is 0, around 39%. The bell-shaped curve represents the density of plausible values from all the models where the coefficient was non-zero. This is scaled so that the height of the density for non-zero values is the probability that the coefficient is non-zero. For mom's IQ, the probability that the coefficient is non-zero is quite small, so no vertical bar is present. The range of plausible values is centered far from 0, also reflecting our beliefs after seeing the data that this variable is important. Mom's age has a much higher probability of being 0, hence the higher bar. And even for the models where it is forced into the model, the distribution overlaps 0. 
-->

### Summary

<!--
We have shown how Bayesian model averaging can be used to address model uncertainty using the ensemble of models for inference, rather than selecting a single model. We've applied this to the kid's cognitive score example using software in R. After successful completion of this module, you should be able to interpret the output under BMA. 
6:51
In this example we've illustrated the concepts using BIC and of reference prior on the coefficient. 
6:58
In the next collection of videos, we will explore alternative prior distributions as part of prior sensitivity. And we'll look at algorithms so explore the space of models when it is no longer possible to enumerate all possible models. 
-->
