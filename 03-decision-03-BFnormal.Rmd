## Hypothesis Testing with Normal Populations

In Section \@ref(sec:bayes-factors), we described how the Bayes factors can be used for hypothesis testing. Now we will use the Bayes factors to test normal means, i.e. compare two groups of normally-distributed populations. We divide this mission into four cases: known variance, unknown variance, paired data, and independent groups.

### Bayes Factors for Testing a Normal Mean: variance known

```{r setup, echo=FALSE, include=FALSE}
library(knitr)
library(ggplot2)
library(latex2exp)

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#000000", "#009E73", "#0072B2", "#D55E00", "#CC79A7")

bf12 = function(n, Z, n0=1) {
return(exp(.5*(log(n + n0) - log(n0) - n*Z^2/(n + n0))))}
postprob = function(x){return(1/(1 + 1/x))}

bf12_Cauchy = function(n, Z, r=1) {
 int = integrate(f =function(x, n, z) {
   dgamma(x, shape=.5, rate=r^2/2)/bf12(n,z,x)}, lower=0, upper =Inf, n=n,z=Z)
 return(1/int$value)
}

bf12_point = function(ybar, m_1, m_2, n=100, sigma=1) {
  bf = exp(dnorm(ybar, m_1, sigma/sqrt(n), log=T) -
           dnorm(ybar, m_2, sigma/sqrt(n), log=T))
  return(bf)
  }
```

Now we show how to obtain base factors for testing hypothesis about a normal mean, where **the variance is known**. To start, let's consider a random sample of observations from a normal population with mean $\mu$ and pre-specified variance $\sigma^2$. We consider testing whether the population mean $\mu$ is equal to $m_0$ or not. 

Therefore, we can formulate the data and hypotheses as below:

**Data**
$$Y_1, \cdots, Y_n \iid \No(\mu, \sigma^2)$$

**Hypotheses**

* $H_1: \mu = m_0$
* $H_2: \mu \neq m_0$

**Priors**

We also need to specify priors for $\mu$ under both hypotheses. Under $H_1$, we assume that $\mu$ is exactly $m_0$, so this occurs with probability 1 under $H_1$. Now under $H_2$, $\mu$ is unspecified, so we describe our prior uncertainty with the conjugate normal distribution centered at $m_0$ and with a variance $\sigma^2/\mathbf{n_0}$. This is centered at the hypothesized value $m_0$, and it seems that the mean is equally likely to be larger or smaller than $m_0$, so a dividing factor $n_0$ is given to the variance. The hyper parameter $n_0$ controls the precision of the prior as before.

In mathematical terms, the priors are:

* $H_1: \mu = m_0  \text{  with probability 1}$
* $H_2: \mu \sim \No(m_0, \sigma^2/\mathbf{n_0})$

**Bayes Factor**

Now the Bayes factor for comparing $H_1$ to $H_2$ is the ratio of the distribution, the data under the assumption that $\mu = m_0$ to the distribution of the data under $H_2$.

$$\begin{aligned}
\BF[H_1 : H_2] &= \frac{p(\data \mid \mu = m_0, \sigma^2 )}
 {\int p(\data \mid \mu, \sigma^2) p(\mu \mid m_0, \mathbf{n_0}, \sigma^2)\, d \mu} \\ 
\BF[H_1 : H_2] &=\left(\frac{n + \mathbf{n_0}}{\mathbf{n_0}} \right)^{1/2} \exp\left\{-\frac 1 2 \frac{n }{n + \mathbf{n_0}} Z^2 \right\} \\
 Z   &=  \frac{(\bar{Y} - m_0)}{\sigma/\sqrt{n}}
\end{aligned}$$

The term in the denominator requires integration to account for the uncertainty in $\mu$ under $H_2$. And it can be shown that the Bayes factor is a function of the observed sampled size, the prior sample size $n_0$ and a $Z$ score.

Let's explore how the hyperparameters in $n_0$ influences the Bayes factor in Equation \@ref(eq:BayesFactor). For illustration we will use the sample size of 100. Recall that for estimation, we interpreted $n_0$ as a prior sample size and considered the limiting case where $n_0$ goes to zero as a non-informative or reference prior. 

\begin{equation}
\BF[H_1 : H_2] = \left(\frac{n + \mathbf{n_0}}{\mathbf{n_0}}\right)^{1/2} \exp\left\{-\frac{1}{2} \frac{n }{n + \mathbf{n_0}} Z^2 \right\}
(\#eq:BayesFactor)
\end{equation}

Figure \@ref(fig:vague-prior) shows the Bayes factor for comparing $H_1$ to $H_2$ on the y-axis as $n_0$ changes on the x-axis. The different lines correspond to different values of the $Z$ score or how many standard errors $\bar{y}$ is from the hypothesized mean. As expected, larger values of the $Z$ score favor $H_2$.

```{r vague-prior, fig.cap="Vague prior for mu: n=100",fig.height=5, echo=FALSE}
myblue = rgb(86,155,189, name="myblue", max=256)
mydarkgrey = rgb(.5,.5,.5, name="mydarkgrey", max=1)
Z = c(1.65, 1.96, 2.81, 3.62 )
nsim = 5000
n0  = seq(0, 1, length=nsim)
#n0 = 1
n  = c(25, 50,100, 10000)

bf.C = c(bf12_Cauchy(n[1], Z[1]), bf12_Cauchy(n[2], Z[2]),
         bf12_Cauchy(n[1], Z[3]),bf12_Cauchy(n[2], Z[4]))
df = data.frame(Z = rep(as.character(Z), rep(nsim, 4)),
                bf = c(bf12(n[2],Z[1], n0), bf12(n[2],Z[2], n0),
                       bf12(n[2],Z[3], n0), bf12(n[2],Z[4], n0)),
                bf.C = rep(bf.C, rep(nsim, 4)),
                n0 =c(n0, n0, n0, n0))
bfplot = ggplot(df, aes(x=n0, y=bf, group=Z, colour=Z, linetype=Z)) +
              scale_colour_manual(values=cbPalette) +
              geom_line() + geom_abline(slope=0, intercept=0) + scale_y_log10() +
              xlab(expression(n[0])) + ylab("BF[H1:H2]") +
               theme(panel.background = element_rect(fill = "transparent", colour = NA),
                 legend.key = element_rect(colour = "transparent", fill = NA),
                  plot.background = element_rect(fill = "transparent", colour = NA)) +
                     theme(legend.position=c(.75, .75)) +
  theme(text = element_text(size=20))

#legend(30, .95, legend=paste("Z = ", as.character(Z), "p-value = ",
#                             as.character(round(2*pnorm(-Z),4))),
#                             lty=1:4, lwd=rep(3,4),
#                             col=rep(myblue,4))
#legend(.6, 7 , col=c(myblue, "darkgrey"), lwd=rep(2,2),  lty=c(1,2),
#       legend=c("male", "prior"))

bfplot
```

But as $n_0$ becomes smaller and approaches 0, the first term in
the Bayes factor goes to infinity, while the exponential term involving the
data goes to a constant and is ignored. In the limit as $n_0 \rightarrow 0$ under this noninformative prior, the Bayes factor paradoxically ends up favoring $H_1$ regardless of the value of $\bar{y}$.

The takeaway from this is that we cannot use improper priors with $n_0 = 0$, if we are going to test our hypothesis that $\mu = n_0$. Similarly, vague priors that use a small value of $n_0$ are not recommended due to the sensitivity of the results to the choice of an arbitrarily small value of $n_0$.

This problem arises with vague priors -- the Bayes factor favors the null model $H_1$ even when the data are far away from the value under the null -- are known as the Bartlett's paradox or the Jeffrey's-Lindleys paradox.

Now one way to try to understand the effect of prior is through the standard effect size 

$$\delta = \frac{\mu - m_0}{\sigma}.$$
The prior of the standard effect size is 

$$\delta \mid   H_2  \sim \No(0, \frac{1}{\mathbf{n_0}})$$

This allows us to think about a standardized effect independent of the units of the problem. One default choice is using the unit information prior, where the prior sample size $n_0$ is 1, leading to a standard normal for the standardized effect size. This is depicted with the blue normal density in Figure \@ref(fig:effect-size).

```{r effect-size, fig.cap="Prior on standard effect size", echo=FALSE}
x = seq(-6,5, length=1000)
par(cex=1.55, cex.axis=1.5, cex.lab=2, mar=c(5, 5, 2, 2), col.lab=mydarkgrey, col.axis=mydarkgrey, col=mydarkgrey)
plot(x, dnorm(x) , type="l", lwd=3, ylab="density", xlab=expression(delta), col=myblue, ylim=c(0, .9), bty="n")
#lines(x, dt(x, df=1), lwd=3, col="orange")
lines(x, dnorm(x, sd=.5), lwd=3,  lty = 2, col="orange")
#lines(x, dt(x/.5, df=1)/.5, lwd=3,   lty=2, col="orange")
legend(x="topleft", legend=c("N(0,1)", "N(0, .25)"),
       col=c(myblue, "orange"),
       lty=c(1,2), lwd=3, bty="n")
```

UNFINISHED BELOW

### Bayes Factors for Testing a Normal Mean: unknown variance

### Testing Normal Means: paired data

### Testing Normal Means: independent groups

### Inference after Testing
