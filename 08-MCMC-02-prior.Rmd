## Priors for Bayesian Model Uncertainty

So far, we have discussed Bayesian model selection and Bayesian model averaging using BIC. BIC is an asymptotic approximation of the log of marginal likelihood of models, when the number of data points is large. Under BIC, prior distribution of $\vec{\beta}$ is uniformaly flat, which is the same as applying the reference prior on $\vec{\beta}$ and $\sigma$. In this section, we will introduce a new conjugate prior distribution, called the Zellner's $g$-prior. We will see that this prior leads to simple expressions for the Bayes factors, in terms of summary statistics from ordinary least square (OLS). We will talk about choosing the parameter $g$ in the prior and conduct a sensitivity analysis, using the kid's cognitive score data that we used in earlier sections.

### Zellner's $g$-Prior

It is more convenient for us to rewrite the multiple regression model centering all the variables to derive the Zellner's $g$-prior conjugacy. Let $y_1,\cdots,y_n$ to be the observations. Our multiple regression model is

$$ y_i = \beta_0 + \beta_1(x_{1,i}-\bar{x}_1) + \beta_2(x_{2, i}-\bar{x}_2)+\cdots +\beta_p(x_{p, i}-\bar{x}_p)+\epsilon_i, \quad 1\leq i\leq n.$$

As before, $\bar{x}_1, \cdots,\bar{x}_p$, are the sample means of the variables $X_1,\cdots,X_p$. Since we have centered all the variables, $\beta_0$ is no longer the $y$-intercept. Instead, it is the sample mean of $Y$ when taking $X_1=\bar{x}_1,\cdots, X_p=\bar{x}_p$. $\beta_1,\cdots,\beta_p$ are the coefficients for the $p$ variables. $\vec{\beta}=(\beta_0,\beta_1,\cdots,\beta_p)$ is the vector notation representing all coefficients, including $\beta_0$.

Under this model, we assume

$$ y_i~|~ \vec{\beta}, \sigma^2~\overset{\text{iid}}{\sim}~\text{N}(\beta_0+\beta_1(x_{1,i}-\bar{x}_1)+\cdots+\beta_p(x_{p,i}-\bar{x}_p), \sigma^2), $$
which is equivalent to
$$ \epsilon_i~|~ \vec{\beta}, \sigma^2 ~\overset{\text{iid}}{\sim}~\text{N}(0, \sigma^2). $$


We then specify the prior distributions for $\beta_j,\ 0\leq j\leq p$. Zellner proposed a simple informative conjugate multivariate normal prior for $\vec{\beta}$ conditioning on $\sigma^2$ as

$$ \vec{\beta}~|~\sigma^2 ~\sim ~\text{N}(\vec{b}_0, \Sigma = g\sigma^2S_{\bf{xx}}^{-1}), $$

where 
$$ S_{\bf{xx}} = (\mathbf{X}-\bar{\mathbf X})^T(\mathbf X - \bar{\mathbf X}), $$
if we let $\mathbf{X} = [\bf{1}, X_1,\cdots,X_p]$ to be the feature matrix and $\bar{\mathbf{X}}$ be the matrix consisting of the column sample means in each column. If you recall, $S_{\bf{xx}}$ in simple linear regression is just $\displaystyle \sum_{i=1}^n(x_i-\bar{x})^2$, the sum of squares for single $X$. Here in multiple regression, $S_{\bf{xx}}$ provides the variance and covariance for OLS. 

The parameter $g$ scales the prior variance of $\vec{\beta}$, over the OLS variances $\sigma^2S_{\bf{xx}}^{-1}$.  The advantage of this prior is, it reduces prior elicitation down to two components; the prior mean $\vec{b}_0$ and the scalar $g$. We use $g$ to control the size of the variance of the prior, rather than set separate priors for all the variances and covariances (there would be a lot, considering this is a multivariate normal distribution).

The advantage of using Zellner's $g$-prior is that it leads to simple updating rules, like all conjugate priors. Moreover, the posterior mean and posterior variance have simple forms. The posterior mean is
$$ \frac{g}{1+g}\vec{\hat{\beta}} + \frac{1}{1+g}\vec{b}_0, $$
where $\vec{\hat{\beta}}$ is the frequentist OLS estimates of coefficients. The posterior variance is
$$  \frac{g}{1+g}\sigma^2S_{\bf{xx}}^{-1}. $$

From the posterior mean formula, we can see that the posterior mean is a weighted average of the prior mean $\vec{b}_0$ and the OLS estimates $\vec{\hat{\beta}}$. Since $\displaystyle \frac{g}{1+g}$ is strictly less than 1, the Zellner's $g$-prior shrinks the OLS estimates $\vec{\hat{\beta}}$ towards the prior mean $\vec{b}_0$. As $g\rightarrow \infty$, $\displaystyle \frac{g}{1+g}\rightarrow 1$ and $\displaystyle \frac{1}{1+g}\rightarrow 0$, and we recover the OLS estimates as in the reference prior.

Similarly, the posterior variancc is a shrunken version of the OLS variance, by a factor $\displaystyle \frac{g}{1+g}$. The posterior distribution of $\vec{beta}$ has a normal distribution
$$ \vec{\beta}~|~\sigma^2, \text{data}~\sim~ \text{N}(\frac{g}{1+g}\vec{\hat{\beta}} + \frac{1}{1+g}\vec{b}_0,\ \frac{g}{1+g}\sigma^2S_{\bf{xx}}^{-1}). $$

Because of this simplicity, Zellner's $g$-prior has been widely used in Bayesian model selection and Bayesian model averaging. One of the most popular versions uses the g prior for all coefficients except the intercept, and takes the prior mean $\vec{b}_0 = \vec{0}$. If we are not testing any hypotheses about the intercept, we may combine this $g$-prior with the reference prior for the intercept $\beta_0$ and $\sigma^2$, that is, we set
$$ p(\beta_0, \sigma^2) \propto \frac{1}{\sigma^2}. $$

Under this prior, the Bayes factor for comparing model $M_m$ to the null model $M_0$, which only has the intercept, is simply
$$ \text{BF}[M_m:M_0] = (1+g)^{(n-p_m-1)/2}(1+g(1-R_m^2))^{-(n-1)/2}. $$

Here $p_m$ is the number of predictors in $M_m$, $R_m^2$ is the $R$-squared of model $M_m$.

Using the Bayes factor, we can compare any two models using the posterior odds. For example, we can compare model $M_m$ with the null model $M_0$ by
$$ \frac{p(M_m~|~\text{data}, g)}{p(M_0~|~\text{data}, g)} = \text{BF}[M_m:M_0]\frac{p(M_m)}{p(M_0)}. $$


Now the question is, how do we pick $g$? As we see that, the Bayes factor depends on $g$. If $g\rightarrow \infty$,  $\text{BF}[M_m:M_0]\rightarrow 0$. This provides overwhelming evidence against model $M_m$, no matter how many predictors we pick for $M_m$ and the data. This is the Bartlett's/Jeffrey-Lindley's paradox.

On the other hand, if we use any arbitrary fixed value of $g$, and include more and more predictors. $R$-squared $R_m^2$ will get closer and closer to 1, but the Bayes factor will maintain bounded. With $R_m^2$ getting larger and larger, we would expect the alternative model $M_m$ would be supported. However, a bounded Bayes factor would not provide overwhelming support for $M_m$, even in the frequentist approach we are getting better and better fit for the data. This is the information paradox, when the Bayes factor comes to a different conclusions. 

<!--
However, there's some solutions that appear to lead to reasonable results in small and large samples. Based on empirical results with real data to theory and provide a resolution to these two paradoxes. In the cases below, the prior distribution depends on n. Now, that might be troubling but remember, that the sum of squares term in the prior is likely getting larger with n. So, this serves to balance the growth of that term. And in the limit converges to prior that depends on the variance of x from the population. 
5:25
The unit information prior takes g = n, so that the information the prior is worth a single observation. This is the same as saying n over g = one. Taking g = n though ignores our uncertainty in the choice of g. Since we don't know g a priori, we could use a prior distribution where the expected value of n over g is 1. One such example is the prior. This is obtained by putting a gamma prior on n over g. A third example puts the beta distribution on one over one plus g over n. The name of hyper g over n is, because the base factor can be expressed in terms of a special hyper g metric function. 
6:07
-->

### Kid's Cognitive Score Example

<!--
Let's look at these priors and a couple of others for the kids cognitive scores example. For each prior, I found the posture of probabilities that each of the four productive variables would be included in the model. This is the posterior probability that each beta equals zero. 
6:24
Each bar plot corresponds to one of the four predictor variables. High school status, mom's IQ. Whether the mom worked during the first three years and mom's age. From left to write in each bar plot, we have the posterior inclusion probabilities under BIC. The g prior, which equals n. The Zellner shell prior. The hyper g over n prior. The empirical Bayes method, where we estimate g and AIC. Where the AIC values are converted to posterior probabilities, like we did with BIC. 
6:55
All the methods agree that we should include mom's IQ. 
6:59
Mom's high school status has a probability included that is over 0.5 across all the methods. With BIC the g prior being the most conservative. Also, all approaches are great at the probabilities that worker A should be included are less than 0.5. With AIC being the least conservative and the AC and the G prior being the most conservative. AIC is designed to find good predictive models and including a predictor whose coefficient is really 0. Does not impact predictions, although a simpler model may be true. In this case, the methods all agree loosely on what are the most important variables. But differ in the magnitudes of the posterior inclusion probabilities. The Zone E Kal Sochi Prior and the Hyper G over N Prior provide results that are between the two extremes of BIC and AIC. And overall perform well in a range a problems. 
7:52
To recap, we've introduced Zellner's g prior a conjugate prior distribution for basic model selection and model averaging. We've discussed some of the problems with choosing g. Where g's that are too large or small lead to inconsistent behavior from what one might expect. To resolve the Bartlett's paradox and information paradox, we recommend g equals n. Or, placing a prior distribution on g over n. 
8:18
We compare posterior inclusion probabilities using a range of priors. Overall, priors agree on the most important variables. 
8:27
In the next videos, we will put everything together with the larger example. And we'll talk more about decision making and what to report in the presence of model and certainty. 

-->