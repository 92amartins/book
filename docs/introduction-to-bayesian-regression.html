<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics</title>
  <meta name="description" content="This book is a written companion for the Coursera Course ‘Bayesian Statistics’ from the Statistics with R specialization.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://www.coursera.org/learn/bayesian/home/info/" />
  <meta property="og:image" content="http://www.coursera.org/learn/bayesian/home/info/cover.png" />
  <meta property="og:description" content="This book is a written companion for the Coursera Course ‘Bayesian Statistics’ from the Statistics with R specialization." />
  <meta name="github-repo" content="StatsWithR/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics" />
  
  <meta name="twitter:description" content="This book is a written companion for the Coursera Course ‘Bayesian Statistics’ from the Statistics with R specialization." />
  <meta name="twitter:image" content="http://www.coursera.org/learn/bayesian/home/info/cover.png" />

<meta name="author" content="Christine Chai">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction-to-losses-and-decision-making.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html"><i class="fa fa-check"></i><b>1</b> The Basics of Bayesian Statistics</a><ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i><b>1.1</b> Bayes’ Rule</a><ul>
<li class="chapter" data-level="1.1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:bayes-rule"><i class="fa fa-check"></i><b>1.1.1</b> Conditional Probabilities &amp; Bayes’ Rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:diagnostic-testing"><i class="fa fa-check"></i><b>1.1.2</b> Bayes’ Rule and Diagnostic Testing</a></li>
<li class="chapter" data-level="1.1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-updating"><i class="fa fa-check"></i><b>1.1.3</b> Bayes Updating</a></li>
<li class="chapter" data-level="1.1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayesian-vs.frequentist-definitions-of-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian vs. Frequentist Definitions of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion"><i class="fa fa-check"></i><b>1.2</b> Inference for a Proportion</a><ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-frequentist-approach"><i class="fa fa-check"></i><b>1.2.1</b> Inference for a Proportion: Frequentist Approach</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-bayesian-approach"><i class="fa fa-check"></i><b>1.2.2</b> Inference for a Proportion: Bayesian Approach</a></li>
<li class="chapter" data-level="1.2.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#effect-of-sample-size-on-the-posterior"><i class="fa fa-check"></i><b>1.2.3</b> Effect of Sample Size on the Posterior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.bayesian-inference"><i class="fa fa-check"></i><b>1.3</b> Frequentist vs. Bayesian Inference</a><ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.bayesian-inference-1"><i class="fa fa-check"></i><b>1.3.1</b> Frequentist vs. Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#continuous-variables-and-eliciting-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Continuous Variables and Eliciting Probability Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-the-discrete-to-the-continuous"><i class="fa fa-check"></i><b>2.1.1</b> From the Discrete to the Continuous</a></li>
<li class="chapter" data-level="2.1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#elicitation"><i class="fa fa-check"></i><b>2.1.2</b> Elicitation</a></li>
<li class="chapter" data-level="2.1.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugacy"><i class="fa fa-check"></i><b>2.1.3</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#three-conjugate-families"><i class="fa fa-check"></i><b>2.2</b> Three Conjugate Families</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#inference-on-a-binomial-proportion"><i class="fa fa-check"></i><b>2.2.1</b> Inference on a Binomial Proportion</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-gamma-poisson-conjugate-families"><i class="fa fa-check"></i><b>2.2.2</b> The Gamma-Poisson Conjugate Families</a></li>
<li class="chapter" data-level="2.2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sec:normal-normal"><i class="fa fa-check"></i><b>2.2.3</b> The Normal-Normal Conjugate Families</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals-and-predictive-inference"><i class="fa fa-check"></i><b>2.3</b> Credible Intervals and Predictive Inference</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-conjugate-priors"><i class="fa fa-check"></i><b>2.3.1</b> Non-Conjugate Priors</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.3.2</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#predictive-inference"><i class="fa fa-check"></i><b>2.3.3</b> Predictive Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html"><i class="fa fa-check"></i><b>3</b> Introduction to Losses and Decision-making</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#losses-and-decision-making"><i class="fa fa-check"></i><b>3.1</b> Losses and Decision Making</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#loss-functions"><i class="fa fa-check"></i><b>3.1.1</b> Loss Functions</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#working-with-loss-functions"><i class="fa fa-check"></i><b>3.1.2</b> Working with Loss Functions</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#minimizing-expected-loss-for-hypothesis-testing"><i class="fa fa-check"></i><b>3.1.3</b> Minimizing Expected Loss for Hypothesis Testing</a></li>
<li class="chapter" data-level="3.1.4" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:bayes-factors"><i class="fa fa-check"></i><b>3.1.4</b> Posterior Probabilities of Hypotheses and Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#inference-and-decision-making-with-multiple-parameters"><i class="fa fa-check"></i><b>3.2</b> Inference and Decision-Making with Multiple Parameters</a><ul>
<li class="chapter" data-level="3.2.1" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:normal-gamma"><i class="fa fa-check"></i><b>3.2.1</b> The Normal-Gamma Conjugate Family</a></li>
<li class="chapter" data-level="3.2.2" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:NG-MC"><i class="fa fa-check"></i><b>3.2.2</b> Monte Carlo Inference</a></li>
<li class="chapter" data-level="3.2.3" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:NG-predictive"><i class="fa fa-check"></i><b>3.2.3</b> Predictive Distributions</a></li>
<li class="chapter" data-level="3.2.4" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:NG-reference"><i class="fa fa-check"></i><b>3.2.4</b> Reference Priors</a></li>
<li class="chapter" data-level="3.2.5" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:NG-Cauchy"><i class="fa fa-check"></i><b>3.2.5</b> Mixtures of Conjugate Priors</a></li>
<li class="chapter" data-level="3.2.6" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#sec:NG-MCMC"><i class="fa fa-check"></i><b>3.2.6</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#hypothesis-testing-with-normal-populations"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Testing with Normal Populations</a><ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#bayes-factors-for-testing-a-normal-mean-variance-known"><i class="fa fa-check"></i><b>3.3.1</b> Bayes Factors for Testing a Normal Mean: variance known</a></li>
<li class="chapter" data-level="3.3.2" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#bayes-factors-for-testing-a-normal-mean-unknown-variance"><i class="fa fa-check"></i><b>3.3.2</b> Bayes Factors for Testing a Normal Mean: unknown variance</a></li>
<li class="chapter" data-level="3.3.3" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#testing-normal-means-paired-data"><i class="fa fa-check"></i><b>3.3.3</b> Testing Normal Means: paired data</a></li>
<li class="chapter" data-level="3.3.4" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#testing-normal-means-independent-groups"><i class="fa fa-check"></i><b>3.3.4</b> Testing Normal Means: independent groups</a></li>
<li class="chapter" data-level="3.3.5" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#inference-after-testing"><i class="fa fa-check"></i><b>3.3.5</b> Inference after Testing</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-losses-and-decision-making.html"><a href="introduction-to-losses-and-decision-making.html#exercises-1"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html"><i class="fa fa-check"></i><b>4</b> Introduction to Bayesian Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#bayesian-simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Bayesian Simple Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#frequentist-ordinary-least-square-simple-linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Frequentist Ordinary Least Square Simple Linear Regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#bayesian-simple-linear-regression-using-reference-prior"><i class="fa fa-check"></i><b>4.1.2</b> Bayesian Simple Linear Regression Using Reference Prior</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#deviations-of-marginal-posterior-distributions-of-alpha-beta-and-sigma2"><i class="fa fa-check"></i><b>4.1.3</b> Deviations of Marginal Posterior Distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#bayesian-multiple-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Multiple Linear Regression</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-bayesian-regression" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Introduction to Bayesian Regression</h1>
<p>In the previous chapter, we introduced Bayesian decision making using posterior probabilities and a variety of loss functions. We discussed how to minimize the expected loss for hypothesis testing. Moreover, we instroduce the concept of Bayes factors and gave some examples of how they can be used in Bayesian hypothesis testing for comparison of two means. We also discussed how to choose appropriate and robust priors. When there is no conjugacy, we applied Markov Chain Monte Carlo simulation to approximate the posterior distributions of parameters of interest.</p>
<p>In this chapter, we will apply Bayesian inference to linear regression. We will first apply Bayesian statistics to simple linear regression models, then generalize the results to multiple linear regression models. We will see when using the reference prior, the posterior means, posterior standard deviations, and credible intervals of the coefficients will coincide with the counterparts in the frequentist ordinary least square (OLS) linear regression models. However, using the Bayes framework, we can now interpret the credible intervals as the probability of the coefficients lie in such intervals.</p>

<div id="bayesian-simple-linear-regression" class="section level2">
<h2><span class="header-section-number">4.1</span> Bayesian Simple Linear Regression</h2>
<p>In this section, we turn to Bayesian inference in simple linear regression. We will use reference prior distribution which will provide a connection between the frequentist solution and Bayesian answers. This provides a baseline analysis for comparions with more informative prior distributions. To illustrate the ideas, we will use an example of predicting body fat.</p>
<div id="frequentist-ordinary-least-square-simple-linear-regression" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Frequentist Ordinary Least Square Simple Linear Regression</h3>
<p>Obtaining accurate measurements of body fat is expensive and not easy to be done. Instaed, predictive models which predict the percentage of body fat using readily available measurements such as abdominal circumference are easy to use and inexpensive. We will illustrate this using the <code>bodyfat</code> data from the library <code>BAS</code>.</p>
<p>To start, we load the <code>BAS</code> library (you may download the package from CRAN) to access the dataframe. We print out a summary of the variables in this dataframe.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(BAS)</code></pre></div>
<pre><code>## Warning: package &#39;BAS&#39; was built under R version 3.4.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(bodyfat)
<span class="kw">summary</span>(bodyfat)</code></pre></div>
<pre><code>##     Density         Bodyfat           Age            Weight     
##  Min.   :0.995   Min.   : 0.00   Min.   :22.00   Min.   :118.5  
##  1st Qu.:1.041   1st Qu.:12.47   1st Qu.:35.75   1st Qu.:159.0  
##  Median :1.055   Median :19.20   Median :43.00   Median :176.5  
##  Mean   :1.056   Mean   :19.15   Mean   :44.88   Mean   :178.9  
##  3rd Qu.:1.070   3rd Qu.:25.30   3rd Qu.:54.00   3rd Qu.:197.0  
##  Max.   :1.109   Max.   :47.50   Max.   :81.00   Max.   :363.1  
##      Height           Neck           Chest           Abdomen      
##  Min.   :29.50   Min.   :31.10   Min.   : 79.30   Min.   : 69.40  
##  1st Qu.:68.25   1st Qu.:36.40   1st Qu.: 94.35   1st Qu.: 84.58  
##  Median :70.00   Median :38.00   Median : 99.65   Median : 90.95  
##  Mean   :70.15   Mean   :37.99   Mean   :100.82   Mean   : 92.56  
##  3rd Qu.:72.25   3rd Qu.:39.42   3rd Qu.:105.38   3rd Qu.: 99.33  
##  Max.   :77.75   Max.   :51.20   Max.   :136.20   Max.   :148.10  
##       Hip            Thigh            Knee           Ankle     
##  Min.   : 85.0   Min.   :47.20   Min.   :33.00   Min.   :19.1  
##  1st Qu.: 95.5   1st Qu.:56.00   1st Qu.:36.98   1st Qu.:22.0  
##  Median : 99.3   Median :59.00   Median :38.50   Median :22.8  
##  Mean   : 99.9   Mean   :59.41   Mean   :38.59   Mean   :23.1  
##  3rd Qu.:103.5   3rd Qu.:62.35   3rd Qu.:39.92   3rd Qu.:24.0  
##  Max.   :147.7   Max.   :87.30   Max.   :49.10   Max.   :33.9  
##      Biceps         Forearm          Wrist      
##  Min.   :24.80   Min.   :21.00   Min.   :15.80  
##  1st Qu.:30.20   1st Qu.:27.30   1st Qu.:17.60  
##  Median :32.05   Median :28.70   Median :18.30  
##  Mean   :32.27   Mean   :28.66   Mean   :18.23  
##  3rd Qu.:34.33   3rd Qu.:30.00   3rd Qu.:18.80  
##  Max.   :45.00   Max.   :34.90   Max.   :21.40</code></pre>
<p>This dataframe includes 252 measurements on men of body fat and other measurements, such as waist circumference (<code>Abdomen</code>). We will use <code>Abdomen</code> to illustrate Bayesian simple linear regression. We regress the response variable <code>Bodyfat</code> on the predictor <code>Abdomen</code>, which gives us the model <span class="math display">\[ y_i = \alpha + \beta x_i + \epsilon_i, \]</span> which the assumption that the errors <span class="math inline">\(\epsilon_i\)</span> are independent and identically distributed as normal random variables with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The figure below shows the percentage body fat obtained from under water weighing and the abdominal circumference for 252 men. To predict body fat, the line overlayed on the scatter plot illustrates the best fitting ordinary least squares line obtained with the <code>lm</code> function in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Bodyfat <span class="op">~</span><span class="st"> </span>Abdomen, <span class="dt">data =</span> bodyfat, 
     <span class="dt">xlab =</span> <span class="st">&quot;abdomen circumference (cm)&quot;</span>, 
     <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)

<span class="co"># Ordinary least square linear regression</span>
bodyfat.lm =<span class="st"> </span><span class="kw">lm</span>(Bodyfat <span class="op">~</span><span class="st"> </span>Abdomen, <span class="dt">data =</span> bodyfat)
<span class="kw">summary</span>(bodyfat.lm)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Bodyfat ~ Abdomen, data = bodyfat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.0160  -3.7557   0.0554   3.4215  12.9007 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -39.28018    2.66034  -14.77   &lt;2e-16 ***
## Abdomen       0.63130    0.02855   22.11   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.877 on 250 degrees of freedom
## Multiple R-squared:  0.6617, Adjusted R-squared:  0.6603 
## F-statistic: 488.9 on 1 and 250 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta =<span class="st"> </span><span class="kw">coef</span>(bodyfat.lm)
<span class="kw">abline</span>(beta, <span class="dt">lwd =</span> <span class="dv">4</span>, <span class="dt">col =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="04-regression-01-Bayesian-simple-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>From the summary, we see that this model has an estimated slope, <span class="math inline">\(\hat{\beta}\)</span>, of 0.63 and an estimated intercept, <span class="math inline">\(\hat{\alpha}\)</span>, of about -39.28%. For every additional centimeter, we expect body fat to increase by 0.63%. The negative interceptive course does not make sense as a physical model, but neither does predicting a male with a waist of zero centimeters. Nevertheless, this linear regression may be an accurate approximation for prediction purposes for measurements that are in the observed range for this population.</p>
<p>The residuals, which provide an estimate of the fitting error, are equal to <span class="math inline">\(\hat{\epsilon}_i = Y_i - \hat{Y}_i\)</span>, the difference between the observed values <span class="math inline">\(Y_i\)</span> and the fited values <span class="math inline">\(\hat{Y}_i = \hat{\alpha} + \hat{\beta}X_i\)</span>, where <span class="math inline">\(X_i\)</span> is the abdominal circumference for the <span class="math inline">\(i\)</span>th male. <span class="math inline">\(\hat{\epsilon}_i\)</span> are used for diagnostics as well as estimating the constant variance in the assumption of the model <span class="math inline">\(\sigma^2\)</span> via the mean squared error (MSE): <span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n-2}\sum \hat{\epsilon}_i^2. \]</span> Here the degrees of freedom <span class="math inline">\(n-2\)</span> are the number of observations adjusted for the number of parameters that we estimated in the regression. The MSE, <span class="math inline">\(\hat{\sigma}^2\)</span>, may be obtained from the output as the square of the entry labeled “residual standard error”.</p>
<p>Since residuals and fitted values are uncorrelated with the expected value of the residuals equal to zero if the model is correct, the scatterplot of residuals versus fitted values provides an additional visual check of the model adequacy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">residuals</span>(bodyfat.lm) <span class="op">~</span><span class="st"> </span><span class="kw">fitted</span>(bodyfat.lm))
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="04-regression-01-Bayesian-simple-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>With the exception of the one observation for the individual with the largest waist measurement, the residual plot suggests that the linear regression is a reasonable approximation.</p>
<p>Furthermore, we can check the normal probability plot of the residuals for the assumption of normally distributed errors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(bodyfat.lm, <span class="dt">which =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="04-regression-01-Bayesian-simple-regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="bayesian-simple-linear-regression-using-reference-prior" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Bayesian Simple Linear Regression Using Reference Prior</h3>
<p>Let us now turn to the Bayesian version and show how to obtain the posterior distributions of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> under the reference prior.</p>
<p>The Bayesian model starts with the same model as the classical frequentist approach: <span class="math display">\[ y_i = \alpha + \beta x_i + \epsilon_i, \]</span> with the assumption that the errors, <span class="math inline">\(\epsilon_i\)</span>, are independent and identically distributed as normal random variables with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>. This assumption is exactly the same as the classical inference for testing and constructing confidence intervals for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>Our goal is to update the distributions of the unknown parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>, based on the data <span class="math inline">\(x_1, y_1, \cdots, x_n, y_n\)</span>, where <span class="math inline">\(n\)</span> is the number of observations. We may center the covariate in order to simplify our later calculations. Here we set <span class="math inline">\(\alpha^* = \alpha + \beta \bar{x}\)</span>, where <span class="math inline">\(\bar{x}\)</span> is the mean of all <span class="math inline">\(x_i\)</span>’s. Then we can rewrite <span class="math inline">\(y_i\)</span> as <span class="math display">\[ y_i = \alpha^* + \beta(x_i-\bar{x}) + \epsilon_i. \]</span> We will first find the posterior distribution of <span class="math inline">\(\alpha^*\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>, then back solve the posterior distribution of <span class="math inline">\(\alpha\)</span>.</p>
<p>Under the assumption that the errors <span class="math inline">\(\epsilon_i\)</span> are normally distributed with constant variance <span class="math inline">\(\sigma^2\)</span>, we have for each response <span class="math inline">\(y_i\)</span>, conditioning on the observed data <span class="math inline">\(x_i\)</span> and the parameters <span class="math inline">\(\alpha^*,\ \beta,\ \sigma^2\)</span>, is normally distributed: <span class="math display">\[ y_i~|~x_i, \alpha^*, \beta,\sigma^2 \sim \mathcal{N}(\alpha^* + \beta(x_i-\bar{x}), \sigma^2),\qquad i = 1,\cdots, n. \]</span> That is, the likelihood of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i, \alpha^*, \beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> is <span class="math display">\[ \pi(y_i~|~x_i, \alpha^*, \beta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i-(\alpha^*+\beta (x_i-\bar{x})))^2}{2\sigma^2}\right). \]</span></p>
<p>We will first consider the standard noninformative prior (reference prior) which gives the analogue to the frequentist results. Here we assume <span class="math display">\[ \pi(\alpha^*, \beta, \sigma^2)\propto \frac{1}{\sigma^2}. \]</span> Using the hierachical model, we may equivalently assume that <span class="math display">\[ \pi(\alpha^*, \beta~|~\sigma^2) \propto 1, \qquad \pi(\sigma^2) \propto \frac{1}{\sigma^2}, \]</span> which will give the same joint prior distribution. Then we apply the Bayes’ rule to derive the posterior joint distribution after observing <span class="math inline">\(y_1,\cdots, y_n\)</span>: <span class="math display">\[
\begin{aligned}
\pi^*(\alpha^*, \beta, \sigma^2~|~y_1,\cdots,y_n) \propto &amp; \left[\prod_i^n\pi(y_i~|~x_i,\alpha^*,\beta,\sigma^2)\right]\pi(\alpha^*, \beta,\sigma^2) \\
\propto &amp; \left[\left(\frac{1}{(\sigma^2)^{1/2}}\exp\left(-\frac{(y_1-(\alpha^*+\beta x_1 - \beta \bar{x}))^2}{2\sigma^2}\right)\right)\times\cdots \right.\\
&amp; \left. \times \left(\frac{1}{(\sigma^2)^{1/2}}\exp\left(-\frac{(y_n-(\alpha^* +\beta x_n-\beta \bar{x}))^2}{2\sigma^2}\right)\right)\right]\times\left(\frac{1}{\sigma^2}\right)\\
\propto &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\sum_i\left(y_i-\alpha^*-\beta (x_i-\bar{x})\right)^2}{2\sigma^2}\right)
\end{aligned}
\]</span> Recall that the residual sum of squares (SSR) is defined to be <span class="math display">\[ \text{SSR} = \sum_i^n (y_i - \hat{y}_i)^2 = \sum_i^n \left(y_i - \hat{\alpha^*} - \hat{\beta} (x_i-\bar{x})\right)^2, \]</span> where <span class="math inline">\(\hat{\alpha^*}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are the regression coefficients under the frequentist method: <span class="math display">\[ \hat{\beta} = \frac{\sum_i (x_i-\bar{x})(y_i-\bar{y})}{\sum_i (x_i-\bar{x})^2},\qquad \hat{\alpha^*} = \hat{\alpha} + \hat{\beta}\bar{x} = \bar{y}-\hat{\beta}\bar{x} + \hat{\beta}\bar{x} = \bar{y}. \]</span> Here <span class="math inline">\(\bar{y}\)</span> is the mean of <span class="math inline">\(y_1,\cdots,y_n\)</span>.</p>
<p>(????? Need to write a shorter version, then the longer one gives the detailed calculations)</p>
</div>
<div id="deviations-of-marginal-posterior-distributions-of-alpha-beta-and-sigma2" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Deviations of Marginal Posterior Distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span></h3>
<p>We may use these quantities to further simplify the numerator inside the exponential function: <span class="math display">\[ 
\begin{aligned}
\sum_i^n \left(y_i - \alpha^* - \beta (x_i-\bar{x})\right)^2 = &amp; \sum_i^n \left(y_i - \hat{\alpha^*} - \hat{\beta}(x_i-\bar{x}) - (\alpha^* - \hat{\alpha^*}) - (\beta - \hat{\beta})(x_i-\bar{x})\right)^2 \\
= &amp; \sum_i^n \left(y_i - \hat{\alpha^*} - \hat{\beta}(x_i-\bar{x})\right)^2 + \sum_i^n (\alpha^* - \hat{\alpha^*})^2 + \sum_i^n (\beta-\hat{\beta})^2(x_i-\bar{x})^2 \\
  &amp; - 2\sum_i^n (\alpha^* - \hat{\alpha^*})(y_i-\hat{\alpha^*}-\hat{\beta}(x_i-\bar{x}))\\
  &amp; - 2\sum_i^n (\beta-\hat{\beta})(x_i-\bar{x})(y_i-\hat{\alpha^*}-\hat{\beta}(x_i-\bar{x}))\\
  &amp; + 2\sum_i^n(\alpha^* - \hat{\alpha^*})(\beta-\hat{\beta})(x_i-\bar{x})\\
= &amp; \text{SSR} + n(\alpha^* - \hat{\alpha^*})^2 + (\beta-\hat{\beta})^2\sum_i^n (x_i-\bar{x})^2 - 2(\alpha^*-\hat{\alpha^*})\sum_i^n (y_i-\hat{\alpha^*}-\hat{\beta}x_i)\\
  &amp; -2(\beta - \hat{\beta})\sum_i^n (x_i-\bar{x})(y_i-\hat{\alpha^*} - \beta(x_i-\bar{x})) + 2(\alpha^*-\hat{\alpha^*})(\beta - \hat{\beta})\sum_i^n(x_i-\bar{x})
\end{aligned}
\]</span></p>
<p>Recall the formula of <span class="math inline">\(\hat{\alpha^*}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>, and that <span class="math inline">\(\displaystyle \sum_i^n (x_i-\bar{x}) = \sum_i^n(y_i-\bar{y}) = 0\)</span>, we have <span class="math display">\[ \sum_i^n (y_i-\hat{\alpha^*} - \hat{\beta}(x_i-\bar{x})) = \sum_i^n (y_i -\bar{y}) - \hat{\beta}\sum_i^n (x_i-\bar{x}) = 0, \]</span> <span class="math display">\[ (\alpha^* - \hat{\alpha^*})(\beta - \hat{\beta})\sum_i^n (x_i - \bar{x}) = 0, \]</span> and finally <span class="math display">\[ \sum_i^n (x_i - \bar{x})(y_i - \hat{\alpha^*} - \hat{\beta}(x_i-\bar{x})) = \sum_i^n (x_i-\bar{x})(y_i-\bar{y}) - \hat{\beta}\sum_i^n (x_i - \bar{x})^2 = 0. \]</span></p>
<p>Therefore, the posterior joint distribution of <span class="math inline">\(\alpha^*, \beta, \sigma^2\)</span> is <span class="math display">\[ 
\begin{aligned}
\pi^*(\alpha^*, \beta,\sigma^2 ~|~y_1,\cdots, y_n) \propto &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\sum_i(y_i - \alpha^* - \beta(x_i-\bar{x}))^2}{2\sigma^2}\right) \\
= &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSR} + n(\alpha^*-\hat{\alpha^*}) + (\beta - \hat{\beta})^2\sum_i (x_i-\bar{x})^2}{2\sigma^2}\right)
\end{aligned}
\]</span></p>
<p>To get the marginal posterior distribution of <span class="math inline">\(\alpha^*\)</span>, we need to integrate out <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[ \pi^*(\alpha^* ~|~y_1,\cdots,y_n) \propto \int_0^\infty \left(\int_{-\infty}^\infty \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSR} + n(\alpha^*-\hat{\alpha^*})+(\beta-\hat{\beta})\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right)\, d\beta\right)\, d\sigma^2 \]</span> The integral inside can be handled as follows: <span class="math display">\[
\begin{aligned}
   &amp; \int_{-\infty}^\infty \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})}{2\sigma^2}\right)\exp\left(-\frac{(\beta-\hat{\beta})\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right)\, d\beta \\
= &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})}{2\sigma^2}\right) \int_{-\infty}^\infty \exp\left(-\frac{(\beta-\hat{\beta})\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right)\, d\beta \\
= &amp; \frac{1}{(\sigma^2)^{(n+2)/2}}\exp\left(-\frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})}{2\sigma^2}\right) \sqrt{2\pi\frac{\sigma^2}{\sum_i(x_i-\bar{x})^2}}\\
\propto &amp; \frac{1}{(\sigma^2)^{(n+1)/2}}\exp\left(-\frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})}{2\sigma^2}\right).
\end{aligned}
\]</span> Here, <span class="math display">\[ \exp\left(-\frac{(\beta - \hat{\beta})^2\sum_i(x_i-\bar{x})^2}{2\sigma^2}\right) \]</span> can be viewed as part of the normal distribution of <span class="math inline">\(\beta\)</span>, with mean <span class="math inline">\(\hat{\beta}\)</span>, and variance <span class="math inline">\(\sigma^2/(\sum_i(x_i-\bar{x})^2)\)</span>. Therefore, the integral is proportional to <span class="math inline">\(\sqrt{\sigma^2/(\sum_i(x_i-\bar{x})^2)}\)</span>.</p>
<p>We then integrate out <span class="math inline">\(\sigma^2\)</span> to get the marginal distribution of <span class="math inline">\(\alpha^*\)</span>. Here we perform change of variable and set <span class="math inline">\(\sigma^2 = \frac{1}{\phi}\)</span>. Then the integral becomes <span class="math display">\[
\begin{aligned}
\pi^*(\alpha^*~|~y_1,\cdots, y_n) \propto &amp; \int_0^\infty \frac{1}{(\sigma^2)^{(n+1)/2}}\exp\left(-\frac{\text{SSR} + n(\alpha^*-\hat{\alpha^*})^2}{2\sigma^2}\right)\, d\sigma^2 \\
\propto &amp; \int_0^\infty \phi^{\frac{n-3}{2}}\exp\left(-\frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})^2}{2}\phi\right)\, d\phi\\
\propto &amp; \left(\frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})^2}{2}\right)^{-\frac{(n-2)+1}{2}}\int_0^\infty s^{\frac{n-3}{2}}e^{-s}\, ds\\
\propto &amp; \left(1+\frac{\left(\frac{\alpha^* - \hat{\alpha^*}}{\hat{\sigma}/\sqrt{n}}\right)}{n-2}\right)^{-\frac{(n-2)+1}{2}}
\end{aligned}
\]</span></p>
<p>Here we use another change of variable by setting <span class="math inline">\(s = \displaystyle \frac{\text{SSR}+n(\alpha^*-\hat{\alpha^*})^2}{2}\phi\)</span>, the fact that <span class="math inline">\(\displaystyle \int_0^\infty s^{(n-3)/2}e^{-s}\, ds\)</span> gives us the Gamma function, which is a constant, and that the mean squared error (MSE) <span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n-2}\sum_i^n \hat{\epsilon}_i^2 = \frac{1}{n-2}\text{SSR} \]</span></p>
<p>The above deviation shows that <span class="math inline">\(\alpha^*\)</span> follows a Student’s <span class="math inline">\(t\)</span>-distribution with center <span class="math inline">\(\hat{\alpha^*} = \bar{y}\)</span>, scale parameter <span class="math inline">\(\displaystyle \frac{\hat{\sigma}^2}{n}\)</span>, and degrees of freedom <span class="math inline">\(n-2\)</span>: <span class="math display">\[ \alpha^* ~|~y_1,\cdots,y_n \sim t_{n-2}\left(\hat{\alpha^*}, \frac{\hat{\sigma}^2}{n}\right) \]</span></p>
<p>A similar approach will lead us to the marginal distribution of <span class="math inline">\(\beta\)</span>. (Tomorrow……)</p>

</div>
</div>
<div id="bayesian-multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">4.2</span> Bayesian Multiple Linear Regression</h2>

<div id="refs" class="references">
<div>
<p>Jeffreys, Sir Harold. 1961. <em>Theory of Probability: 3rd Edition</em>. Clarendon Press.</p>
</div>
<div>
<p>Kass, Robert E, and Adrian E Raftery. 1995. “Bayes Factors.” <em>Journal of the American Statistical Association</em> 90 (430). Taylor &amp; Francis Group: 773–95.</p>
</div>
</div>
</div>
</div>






            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-losses-and-decision-making.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-regression-00-intro.Rmd",
"text": "Edit"
},
"download": ["Bayes_Book.pdf", "Bayes_Book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
